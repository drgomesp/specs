(window.webpackJsonp=window.webpackJsonp||[]).push([[67],{335:function(t,s,e){"use strict";e.r(s);var a=e(13),n=Object(a.a)({},(function(){var t=this,s=t.$createElement,e=t._self._c||s;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("p",[t._v("!!!")]),t._v(" "),e("p",[t._v("This document has "),e("strong",[t._v("moved")]),t._v(".")]),t._v(" "),e("p",[t._v("You'll now find information like this in the "),e("a",{attrs:{href:"https://github.com/ipld/ipld/",target:"_blank",rel:"noopener noreferrer"}},[t._v("ipld/ipld"),e("OutboundLink")],1),t._v(" meta-repo,\nand published to the web at https://ipld.io/ .")]),t._v(" "),e("p",[t._v("All documentation, fixtures, specifications, and web content is now gathered into that repo.\nPlease update your links, and direct new contributions there.")]),t._v(" "),e("p",[t._v("!!!")]),t._v(" "),e("hr"),t._v(" "),e("h1",{attrs:{id:"selector-syntax"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#selector-syntax"}},[t._v("#")]),t._v(" Selector Syntax")]),t._v(" "),e("p",[t._v("This exploration report was originally "),e("a",{attrs:{href:"https://github.com/ipld/specs/pull/239",target:"_blank",rel:"noopener noreferrer"}},[t._v("Pull Requst 239"),e("OutboundLink")],1),t._v(". It got converted "),e("a",{attrs:{href:"https://github.com/vmx/export_issues",target:"_blank",rel:"noopener noreferrer"}},[t._v("via script"),e("OutboundLink")],1),t._v(" into an exploration report in order to preserve all the useful information it contains.")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"_239-add-initial-specification-for-selector-syntax-closed"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_239-add-initial-specification-for-selector-syntax-closed"}},[t._v("#")]),t._v(" #239: Add initial specification for selector syntax. (closed)")]),t._v(" "),e("p",[t._v("Opened 2020-02-06T22:53:50Z by creationix, closed 2020-10-08T21:10:58Z")]),t._v(" "),e("p",[t._v("This is a proposal for a selector syntax that closely models the semantics already in the IPLD format of selectors.  The rational and constraints for the design are included in the documents as well as many examples and hopefully enough description of lexing/parsing behavior to make it unambiguous.")]),t._v(" "),e("p",[t._v("I would love feedback on what you like about this, what drives you crazy, and hopefully find out if this is a good direction.")]),t._v(" "),e("h2",{attrs:{id:"files"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#files"}},[t._v("#")]),t._v(" Files")]),t._v(" "),e("p",[e("code",[t._v("selectors/selector-syntax.md")])]),t._v(" "),e("h1",{attrs:{id:"specification-ipld-selectors-syntax"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#specification-ipld-selectors-syntax"}},[t._v("#")]),t._v(" Specification: IPLD Selectors Syntax")]),t._v(" "),e("p",[e("strong",[t._v("Status: Prescriptive - Draft")])]),t._v(" "),e("h2",{attrs:{id:"introduction"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#introduction"}},[t._v("#")]),t._v(" Introduction")]),t._v(" "),e("h3",{attrs:{id:"motivation-what-is-selectors-syntax"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#motivation-what-is-selectors-syntax"}},[t._v("#")]),t._v(" Motivation - What is Selectors Syntax")]),t._v(" "),e("p",[t._v("*Prerequisites: "),e("RouterLink",{attrs:{to:"/design/history/exploration-reports/selectors.html"}},[t._v("Selectors")]),t._v(".")],1),t._v(" "),e("p",[t._v("IPLD Selectors are represented as IPLD data nodes.  This is great for embedding them in a structured way, but authoring them or viewing them in this format isn't the easiest.  This syntax provides a textual DSL for reading/writing selectors in a more text friendly format.")]),t._v(" "),e("p",[t._v("Tooling can be used to convert between formats and even various styles optimized for the use-case at hand.")]),t._v(" "),e("h4",{attrs:{id:"url-friendly"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#url-friendly"}},[t._v("#")]),t._v(" URL Friendly")]),t._v(" "),e("p",[t._v("Selector syntax should embed easily inside URLs.")]),t._v(" "),e("p",[t._v("This means where possible, this syntax restricts itself to the characters that can be embedded in URLs without needing to escape them. This means this subset of ASCII:")]),t._v(" "),e("div",{staticClass:"language-js extra-class"},[e("pre",{pre:!0,attrs:{class:"language-js"}},[e("code",[e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'!'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"\'"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'('")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("')'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'-'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'0'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'1'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'2'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'3'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'4'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'5'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'6'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'7'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'8'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'9'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'A'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'B'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'C'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'D'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'E'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'F'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'G'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'H'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'I'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'J'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'K'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'L'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'M'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'N'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'O'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'P'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Q'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'R'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'S'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'T'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'U'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'V'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'W'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'X'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Y'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'Z'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'_'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'a'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'b'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'d'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'e'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'g'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'h'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'i'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'j'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'k'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'l'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'m'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'o'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'p'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'q'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'s'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'t'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'v'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'w'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'x'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'y'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'z'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'~'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n")])])]),e("blockquote",[e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-08t18-07-01z-ribasushi"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-08t18-07-01z-ribasushi"}},[t._v("#")]),t._v(" (2020-02-08T18:07:01Z) ribasushi:")]),t._v(" "),e("p",[t._v('"Easy embedding inside URLs" implies "easy visual skimming" ( perhaps with some initial training needed, just like e.g. regular expressions ). Assuming a person reading this is proficient: are we comfortable with a case sensitive, visually-collidable character set?')]),t._v(" "),e("p",[t._v("I am not particularly leaning one way or the other, but rather am bringing the point up for discussion .")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t18-33-03z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t18-33-03z-creationix"}},[t._v("#")]),t._v(" (2020-02-10T18:33:03Z) creationix:")]),t._v(" "),e("p",[t._v("This is certainly something we can use as an added constraint to consider when choosing the characters used for short form.  Currently, it only uses "),e("code",[t._v("f")]),t._v(", "),e("code",[t._v("i")]),t._v(", "),e("code",[t._v("r")]),t._v(", "),e("code",[t._v("u")]),t._v(", "),e("code",[t._v("c")]),t._v(", "),e("code",[t._v("F")]),t._v(", "),e("code",[t._v("*")]),t._v(", "),e("code",[t._v(".")]),t._v(", and "),e("code",[t._v("~")]),t._v(".")]),t._v(" "),e("p",[t._v("The listing of url safe characters is more of a technical constraint about what ASCII characters can be embedded in url components without needing to be escaped.")])]),t._v(" "),e("p",[t._v("This also also means it needs to be as terse as possible and not contain whitespace of any kind.")]),t._v(" "),e("p",[t._v("For example, this selector simulates a git shallow clone by recursively walking commit parents up to depth 5 and walking all of the tree graphs for each.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Starting at the commit block.\nR5f'tree'Rn*~'parents'*~\n")])])]),e("h4",{attrs:{id:"human-friendly"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#human-friendly"}},[t._v("#")]),t._v(" Human Friendly")]),t._v(" "),e("p",[t._v("Selector syntax should be easy to read/author by humans.")]),t._v(" "),e("p",[t._v("This means it should be terser than the JSON or YAML representations of the IPLD data, but still verbose enough to have meaningful structure and keywords/symbols.")]),t._v(" "),e("blockquote",[e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-02t03-49-28z-rvagg"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-02t03-49-28z-rvagg"}},[t._v("#")]),t._v(" (2020-03-02T03:49:28Z) rvagg:")]),t._v(" "),e("p",[t._v("minor style suggestion: collapse these two paragraphs into a bullet-point list hanging off the first sentence:")]),t._v(" "),e("blockquote",[e("p",[t._v("... easy to read/author by humans. This means it should:")]),t._v(" "),e("ul",[e("li",[t._v("Be terser ...")]),t._v(" "),e("li",[t._v("Allow flexibility ...")])])])]),t._v(" "),e("p",[t._v("This means it should allow flexibility with whitespace as well as allowing optional symbols and annotations to make structure easier to see visually.")]),t._v(" "),e("p",[t._v("The exact same selector for git shallow clone from above can also be written in the following style: (This is not another mode, it's the same syntax):")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("recursive(limit=5\n  fields(\n    'tree'(\n      recursive(limit=none\n        all(recurse)\n      )\n    )\n    'parents'(\n      all(recurse)\n    )\n  )\n)\n")])])]),e("h2",{attrs:{id:"examples"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#examples"}},[t._v("#")]),t._v(" Examples")]),t._v(" "),e("h3",{attrs:{id:"deeply-nested-path"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#deeply-nested-path"}},[t._v("#")]),t._v(" Deeply Nested Path")]),t._v(" "),e("p",[t._v("Based on "),e("RouterLink",{attrs:{to:"/design/history/exploration-reports/example-selectors.html#deeply-nested-path"}},[t._v("this example")]),t._v(".")],1),t._v(" "),e("p",[t._v("A selector to extract the year:")]),t._v(" "),e("h4",{attrs:{id:"human-readable-style"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#human-readable-style"}},[t._v("#")]),t._v(" Human Readable Style")]),t._v(" "),e("p",[t._v("This is the default style for human interfacing.  It has clear structure and descriptive keywords.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("fields('characters'(\n  fields('kathryn-janeway'(\n    fields('birthday'(\n      fields('year'(match))\n    ))\n  ))\n))\n")])])]),e("h4",{attrs:{id:"url-embeddable-style"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#url-embeddable-style"}},[t._v("#")]),t._v(" URL Embeddable Style")]),t._v(" "),e("p",[t._v("This is the default style for maximum terseness.  It minifies everything possible.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("f'characters'f'kathryn-janeway'f'birthday'f'year'.\n")])])]),e("h3",{attrs:{id:"getting-a-certain-number-of-parent-blocks-in-a-blockchain"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#getting-a-certain-number-of-parent-blocks-in-a-blockchain"}},[t._v("#")]),t._v(" Getting a certain number of parent blocks in a blockchain")]),t._v(" "),e("p",[t._v("This is based on "),e("RouterLink",{attrs:{to:"/design/history/exploration-reports/example-selectors.html#getting-a-certain-number-of-parent-blocks-in-a-blockchain"}},[t._v("this sample")]),t._v(".")],1),t._v(" "),e("h4",{attrs:{id:"parents-without-recursion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parents-without-recursion"}},[t._v("#")]),t._v(" Parents Without Recursion")]),t._v(" "),e("p",[t._v("Direct and simple path traversal:")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Long Form\nfields('parent'(\n  fields('parent'(\n    fields('parent'(\n      fields('parent'(\n        fields('parent'(\n          match\n        ))\n      ))\n    ))\n  ))\n))\n\n# Short Form\nf'parent'f'parent'f'parent'f'parent'f'parent'.\n")])])]),e("h4",{attrs:{id:"parents-using-recursion"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parents-using-recursion"}},[t._v("#")]),t._v(" Parents Using Recursion")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Long Form\nrecursive(limit=5\n  fields('parent'(\n    recurse\n  ))\n)\n\n# Short Form\nR5f'parent'~\n")])])]),e("h3",{attrs:{id:"getting-changes-up-to-a-certain-one"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#getting-changes-up-to-a-certain-one"}},[t._v("#")]),t._v(" Getting changes up to a certain one")]),t._v(" "),e("p",[t._v("Based on "),e("RouterLink",{attrs:{to:"/design/history/exploration-reports/example-selectors.html#getting-changes-up-to-a-certain-one"}},[t._v("this example")]),t._v(".")],1),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Long Form\nrecursive(\n  limit=100\n  fields(\n    'prev'(recurse)\n  )\n  stopAt=... # Conditions are not specified yet\n)\n\n# Short Form\nR100f'prev'~... # Conditions are not specified yet\n")])])]),e("h3",{attrs:{id:"retrieving-data-recursively"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#retrieving-data-recursively"}},[t._v("#")]),t._v(" Retrieving data recursively")]),t._v(" "),e("p",[t._v("Based on "),e("RouterLink",{attrs:{to:"/design/history/exploration-reports/example-selectors.html#retrieving-data-recursively"}},[t._v("this example")]),t._v(".")],1),t._v(" "),e("p",[t._v("The following selector visits all "),e("code",[t._v("links")]),t._v(" and matches all "),e("code",[t._v("data")]),t._v(" fields:")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Long Form\nrecursive(limit=1000\n  fields(\n    'data'(match)\n    'links'(\n      all(\n        fields('cid'(\n          recurse\n        ))\n      )\n    )\n  )\n)\n\n# Short Form\nR1000f'data'.'links'*f'cid'~\n")])])]),e("h2",{attrs:{id:"syntax-specification"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#syntax-specification"}},[t._v("#")]),t._v(" Syntax Specification")]),t._v(" "),e("p",[t._v("Selectors Syntax is defined as a textual projection of the Selector AST and thus does not contain any of its own runtime semantics.")]),t._v(" "),e("h3",{attrs:{id:"long-and-short-keywords"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#long-and-short-keywords"}},[t._v("#")]),t._v(" Long and Short Keywords")]),t._v(" "),e("p",[t._v("Each selector type has both long and short names that can be used interchangeably as follows:")]),t._v(" "),e("ul",[e("li",[t._v("Matcher can be "),e("code",[t._v("match")]),t._v(" or "),e("code",[t._v(".")])]),t._v(" "),e("li",[t._v("ExploreAll can be "),e("code",[t._v("all")]),t._v(" or "),e("code",[t._v("*")])]),t._v(" "),e("li",[t._v("ExploreFields can be "),e("code",[t._v("fields")]),t._v(" or "),e("code",[t._v("f")])]),t._v(" "),e("li",[t._v("ExploreIndex can be "),e("code",[t._v("index")]),t._v(" or "),e("code",[t._v("i")])]),t._v(" "),e("li",[t._v("ExploreRange can be "),e("code",[t._v("range")]),t._v(" or "),e("code",[t._v("r")])]),t._v(" "),e("li",[t._v("ExploreRecursive can be "),e("code",[t._v("recursive")]),t._v(" or "),e("code",[t._v("R")])]),t._v(" "),e("li",[t._v("ExploreUnion can be "),e("code",[t._v("union")]),t._v(" or "),e("code",[t._v("u")])]),t._v(" "),e("li",[t._v("ExploreConditional can be "),e("code",[t._v("condition")]),t._v(" or "),e("code",[t._v("c")])]),t._v(" "),e("li",[t._v("ExploreRecursiveEdge can be "),e("code",[t._v("recurse")]),t._v(" or "),e("code",[t._v("~")])])]),t._v(" "),e("p",[t._v("This mode-less flexibility, combined with tools to automatically translate in bulk between styles, makes it possible for a single syntax to work well for both human and url embedding use cases.")]),t._v(" "),e("h3",{attrs:{id:"whitespace-is-ignored"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#whitespace-is-ignored"}},[t._v("#")]),t._v(" Whitespace is Ignored")]),t._v(" "),e("p",[t._v("Whitespace is completely ignored by the parser except for inside quoted strings.")]),t._v(" "),e("p",[t._v("Line comments are also ignored even if they contain things that look like quoted strings")]),t._v(" "),e("h3",{attrs:{id:"parentheses-are-usually-optional"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parentheses-are-usually-optional"}},[t._v("#")]),t._v(" Parentheses are Usually Optional")]),t._v(" "),e("p",[t._v("Parentheses annotate structure and are sometimes required for ambigious cases such as unions which contain an arbitrary number of selectors or selectors with optional parameters of conflicting types.")]),t._v(" "),e("p",[t._v("For example "),e("code",[t._v("union(union match match)")]),t._v(" is interpreted as "),e("code",[t._v("union(union(match match))")]),t._v(" and not "),e("code",[t._v("union(union(match) match)")]),t._v(" because in the first, the last match will be part of the inner union and not the outer union.  When minimizing, some parentheses might be kept to preserve semantics.")]),t._v(" "),e("ul",[e("li",[e("code",[t._v("uu..")]),t._v(" -> "),e("code",[t._v("{ selector: { '|': [ { '|': [ { '.': {} }, { '.': {} } ] } ] } }")])]),t._v(" "),e("li",[e("code",[t._v("uu(.).")]),t._v(" -> "),e("code",[t._v("{ selector: { '|': [ { '|': [ { '.': {} } ] }, { '.': {} } ] } }")])])]),t._v(" "),e("p",[t._v("The best practice (and what the default formatting styles will enforce) is for human readable selectors to use parentheses liberally while URL embedding style will only contain the required ones.")]),t._v(" "),e("blockquote",[e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-08t18-23-01z-ribasushi"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-08t18-23-01z-ribasushi"}},[t._v("#")]),t._v(" (2020-02-08T18:23:01Z) ribasushi:")]),t._v(" "),e("p",[t._v("I am not sure I fully understand this / the motivation for "),e("code",[t._v("R")]),t._v("..."),e("code",[t._v("~")]),t._v(' and other scope-pairs is not entirely clear. Could you elaborate  "even harder" why you didn\'t go with a more traditional balanced pair of characters e.g. '),e("code",[t._v("(")]),t._v(" "),e("code",[t._v(")")]),t._v(" ? Especially jarring is the discrepancy of doing couple paragraphs down:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# URL Embeddable\nR(5...)\n")])])]),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t18-03-18z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t18-03-18z-creationix"}},[t._v("#")]),t._v(" (2020-02-10T18:03:18Z) creationix:")]),t._v(" "),e("p",[t._v("I'm not sure what you mean by pairing up "),e("code",[t._v("R")]),t._v(" and "),e("code",[t._v("~")]),t._v(".  They are not necessarily balanced; there could technically be multiple recursive edges inside a single recursive node.  It's not a scope pair, but rather two distinct node types in the selector AST that happen to relate to eachother.")]),t._v(" "),e("p",[t._v("Also I use "),e("code",[t._v("(")]),t._v(" and "),e("code",[t._v(")")]),t._v(" already in the syntax to help with forcing parameters to the correct nodes.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t19-56-55z-ribasushi"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t19-56-55z-ribasushi"}},[t._v("#")]),t._v(" (2020-02-10T19:56:55Z) ribasushi:")]),t._v(" "),e("blockquote",[e("p",[t._v("They are not necessarily balanced; there could technically be multiple recursive edges inside a single recursive node.")])]),t._v(" "),e("p",[t._v("This might "),e("strong",[t._v("very")]),t._v(" well be my lack of understanding of the problem domain. Let's cover this during the ipld meet if time permits.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t21-58-24z-ribasushi"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t21-58-24z-ribasushi"}},[t._v("#")]),t._v(" (2020-02-10T21:58:24Z) ribasushi:")]),t._v(" "),e("p",[t._v("We chatted about this a bit more - I now understand what was meant by the above quote. I withdraw the question about balanced parens 😉")])]),t._v(" "),e("h3",{attrs:{id:"parameters-can-be-named"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parameters-can-be-named"}},[t._v("#")]),t._v(" Parameters can be Named")]),t._v(" "),e("p",[t._v("Parameters can usually be inferred by their contextual position, but there are some cases where it's ambigious and needs to be specified.  There are more cases where it's good to annotate them for human clarity.")]),t._v(" "),e("p",[t._v("For example, "),e("code",[t._v("recursive")]),t._v(" has two required parameters and a 3rd optional one.")]),t._v(" "),e("div",{staticClass:"language-ts extra-class"},[e("pre",{pre:!0,attrs:{class:"language-ts"}},[e("code",[e("span",{pre:!0,attrs:{class:"token function"}},[t._v("recursive")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("sequence"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Selector"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" limit"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" int"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" stopAt"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("?")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" Condition"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Written verbosely with parentheses, named parameters, and whitespace, it looks like this:")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("recursive(\n    limit=5\n    sequence=...\n)\n")])])]),e("p",[t._v("Depending on the context, we could omit the parentheses because the optional "),e("code",[t._v("stopAt")]),t._v(" parameter is of type "),e("code",[t._v("Condition")]),t._v(" and the parser likely expects something else after this node.")]),t._v(" "),e("blockquote",[e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-02t04-26-59z-rvagg"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-02t04-26-59z-rvagg"}},[t._v("#")]),t._v(" (2020-03-02T04:26:59Z) rvagg:")]),t._v(" "),e("p",[t._v("so there aren't any selector forms with possibly ambiguous lengths so predicting a "),e("code",[t._v("stopAt")]),t._v(" in the shortened selector syntax should be straightforward?\nI see "),e("code",[t._v("Matcher")]),t._v(" in the selector schema also has two optional fields, "),e("code",[t._v("onlyIf")]),t._v(" and "),e("code",[t._v("label")]),t._v(", could these get in the way?")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-08t03-02-04z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-08t03-02-04z-creationix"}},[t._v("#")]),t._v(" (2020-03-08T03:02:04Z) creationix:")]),t._v(" "),e("p",[t._v("It's hard to say.  This is why I'm implementing syntax parsers.  So far I've not come across any concrete use cases where the parentheses are actually needed.")])]),t._v(" "),e("p",[t._v("Also we don't need to annotate "),e("code",[t._v("limit=")]),t._v(" or "),e("code",[t._v("sequence=")]),t._v(" since both are non-optional, and unique types.  Notice that the order doesn't matter and we can put "),e("code",[t._v("limit")]),t._v(" before "),e("code",[t._v("sequence")]),t._v(" because of unambigious types.")]),t._v(" "),e("p",[t._v("Best practice is to annotate "),e("code",[t._v("limit")]),t._v(", but not "),e("code",[t._v("sequence")]),t._v(" for human readable, and omit both for URL form.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Human Readable\nrecursive(limit=5 ...)\n# URL Embeddable\nR5...\n")])])]),e("h3",{attrs:{id:"literal-values"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#literal-values"}},[t._v("#")]),t._v(" Literal Values")]),t._v(" "),e("p",[t._v("Some of the selectors accept literal values as parameters.  These are currently "),e("code",[t._v("String")]),t._v(", "),e("code",[t._v("{String:Selector}")]),t._v(", and "),e("code",[t._v("Int")]),t._v(".")]),t._v(" "),e("h4",{attrs:{id:"integers"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#integers"}},[t._v("#")]),t._v(" Integers")]),t._v(" "),e("p",[t._v("Integers can be encoded using base 10 with optional leadin sign:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("123 # Decimal\n-123 # Negative decimal\n")])])]),e("h4",{attrs:{id:"strings"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#strings"}},[t._v("#")]),t._v(" Strings")]),t._v(" "),e("p",[t._v("Strings are quoted using single quote.  They can contain any characters including newlines and unicode characters.  The following characters can be escaped using backslash ("),e("code",[t._v("\\")]),t._v(") followed by a special character.  If the backslash is followed by a character not in the list, it's considered a syntax error.")]),t._v(" "),e("ul",[e("li",[t._v("\\b  Backspace (ascii code 08)")]),t._v(" "),e("li",[t._v("\\f  Form feed (ascii code 0C)")]),t._v(" "),e("li",[t._v("\\n  New line")]),t._v(" "),e("li",[t._v("\\r  Carriage return")]),t._v(" "),e("li",[t._v("\\t  Tab")]),t._v(" "),e("li",[t._v("'  Single quote")]),t._v(" "),e("li",[t._v("\\  Backslash character")])]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("'Hello World'\n'It\\'s a lovely day'\n'Multiline\nstrings'\n'Multiline\\nstrings'\n")])])]),e("h4",{attrs:{id:"maps"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#maps"}},[t._v("#")]),t._v(" Maps")]),t._v(" "),e("p",[t._v("We need to be able to encode the keys for the "),e("code",[t._v("fields")]),t._v(" selector.  This is done using multiple string literals followed by nested contents.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("fields(\n  'foo'(...)\n  'bar'(...)\n)\n")])])]),e("h3",{attrs:{id:"whitespace-and-comments"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#whitespace-and-comments"}},[t._v("#")]),t._v(" Whitespace and Comments")]),t._v(" "),e("p",[t._v("Comments are allowed in this syntax and will be preserved by auto-formatters when possible, but will be stripped when converting to URL style and are not included in the IPLD representation of the selector.")]),t._v(" "),e("p",[t._v("A comment starts at "),e("code",[t._v("#")]),t._v(" and ends at end of line.")]),t._v(" "),e("h2",{attrs:{id:"parser-specification"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parser-specification"}},[t._v("#")]),t._v(" Parser Specification")]),t._v(" "),e("h1",{attrs:{id:"initial-stripping"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#initial-stripping"}},[t._v("#")]),t._v(" Initial Stripping")]),t._v(" "),e("p",[t._v("The parser must act as is there was an initial pass that removed all whitespace not inside strings and all line comments.")]),t._v(" "),e("div",{staticClass:"language-ipldsch extra-class"},[e("pre",{pre:!0,attrs:{class:"language-ipldsch"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# This is a comment 'this is not a string'")]),t._v("\n'This "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# is # a string' this is normal")]),t._v("\nthis is also normal\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Keywords get merged")]),t._v("\nhello world\nhelloworld\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Comments get stripped")]),t._v("\n'a "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("string")]),t._v("' "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# and comment")]),t._v("\n'a "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("string")]),t._v("'\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Strings inside comments are still comments")]),t._v("\nempty "),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# a comment with a 'string'")]),t._v("\nempty\n")])])]),e("h3",{attrs:{id:"identifier-tokenization"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#identifier-tokenization"}},[t._v("#")]),t._v(" Identifier Tokenization")]),t._v(" "),e("p",[t._v("The parser knows a fixed set of built-ins to look for.  This is the long and short forms of the selectors and other built-ins.  To keep the specification simple, text is semantically tokenized by sorting all the identifiers longest first and trying each one in that order till one matches.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# This will match `fields` first and not even try `f`.\nfields...\n")])])]),e("h3",{attrs:{id:"parentheses-and-parse-order"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#parentheses-and-parse-order"}},[t._v("#")]),t._v(" Parentheses and Parse Order")]),t._v(" "),e("p",[t._v("Arguments/parameters are consumed greedily by the innermost consumer.  If the type doesn't match what it is looking for, then it is closed and the next in the stack gets a shot.  If we run out of consumers and the value is unmatched, it's a syntax error.  For example:")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("fields 'fieldName' match\n")])])]),e("p",[t._v("First we parse "),e("code",[t._v("fields")]),t._v(". This expects "),e("code",[t._v("{String:Selector}")]),t._v(", which to the parser, is a stream of alternating "),e("code",[t._v("String")]),t._v(" and "),e("code",[t._v("Selector")]),t._v(" tokens.  We put this on the stack and look at the next value.  It's a "),e("code",[t._v("String")]),t._v(" which has no children.  The consumer on the top of the stack is looking for a string, so we give it to it.  Then we read the next.  It's a "),e("code",[t._v("match")]),t._v(" which also has no children.  The "),e("code",[t._v("fields")]),t._v(" on the stack is now looking for a "),e("code",[t._v("Selector")]),t._v(" which this qualifies as, so it gets consumed next.")]),t._v(" "),e("p",[t._v("After that we reach the end of the stream and pop everything off the stack.  Any consumer that still lacks a required parameter is now a syntax error.")]),t._v(" "),e("p",[t._v("We could have added parentheses to this, but they were not needed since the default parsing interpretation is what we wanted.")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# This is the same as above when parsed.\nfields('fieldname'(match))\n")])])]),e("p",[t._v("When parentheses are added, they can override the default greedy behavior in some otherwise ambigious cases.  Again, the example "),e("code",[t._v("union union match match")]),t._v(" is not "),e("code",[t._v("union(union(match) match)")]),t._v(" but is "),e("code",[t._v("union(union(match match))")]),t._v(" because the innermost union gets to greedily match first.  Extra parentheses can be added as "),e("code",[t._v("union union(match) match)")]),t._v(", or in short form"),e("code",[t._v("uu(.).")]),t._v(" vs "),e("code",[t._v("uu..")])]),t._v(" "),e("h2",{attrs:{id:"known-issues"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#known-issues"}},[t._v("#")]),t._v(" Known issues")]),t._v(" "),e("ul",[e("li",[t._v('Note that the status of this document is "Draft"!')]),t._v(" "),e("li",[t._v('The "Condition" system is not fully specified -- it is a placeholder awaiting further design.')]),t._v(" "),e("li",[t._v("The description of the lexing and parsing algorithm should be sufficient for unambiguous parsing, but more formal consideration is strongly recommended including tools to test for regressions as we add to this language.")])]),t._v(" "),e("h2",{attrs:{id:"other-related-work"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#other-related-work"}},[t._v("#")]),t._v(" Other related work")]),t._v(" "),e("h3",{attrs:{id:"implementations"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#implementations"}},[t._v("#")]),t._v(" Implementations")]),t._v(" "),e("p",[t._v("None yet.")]),t._v(" "),e("h3",{attrs:{id:"design-history"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#design-history"}},[t._v("#")]),t._v(" Design History")]),t._v(" "),e("p",[t._v("None yet.")]),t._v(" "),e("hr"),t._v(" "),e("h2",{attrs:{id:"comments"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#comments"}},[t._v("#")]),t._v(" Comments")]),t._v(" "),e("h4",{attrs:{id:"_2020-02-07t17-01-27z-vmx"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-07t17-01-27z-vmx"}},[t._v("#")]),t._v(" (2020-02-07T17:01:27Z) vmx:")]),t._v(" "),e("p",[t._v("Wow, that is really a good read. My comments are only in regards to typos the rest sounds great.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-08t18-24-18z-ribasushi"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-08t18-24-18z-ribasushi"}},[t._v("#")]),t._v(" (2020-02-08T18:24:18Z) ribasushi:")]),t._v(" "),e("p",[t._v('Marking "request changes" as stand-in for "request discussion". Will d another pass over this once the first two pieces are clarified')]),t._v(" "),e("p",[t._v("Awesome work as a whole!")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-08t18-29-40z-warpfork"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-08t18-29-40z-warpfork"}},[t._v("#")]),t._v(" (2020-02-08T18:29:40Z) warpfork:")]),t._v(" "),e("p",[t._v("I have no objections to this, I think 😃")]),t._v(" "),e("p",[t._v("I'm also not really reviewing for ergonomics though, as I feel ill suited to do so without an application in my mind's eye, which I'm pretty sparse on.  And thus, \"no objections\" is about the greenest light I'm likely to give, if that makes sense. 😛")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t18-43-57z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t18-43-57z-creationix"}},[t._v("#")]),t._v(" (2020-02-10T18:43:57Z) creationix:")]),t._v(" "),e("p",[t._v("After spending a day implementing a full parser, I've discovered the white-space rules are more interesting than I initially thought.  The design goal was to make white-space 100% irrelevant to the parser.  Most programming languages claim to not have significant white-space, but that's not entirely true for any real language.")]),t._v(" "),e("p",[t._v("The first exception obviously is white-space within strings needs to be preserved.  This is easy enough, I simply turn off white-space stripping when within quoted sections.")]),t._v(" "),e("p",[t._v("The second case is one you typically don't realize.  Most languages use white-space as token separators.  For example:")]),t._v(" "),e("div",{staticClass:"language- extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("# Escaped quotes\n'This is ''one'' string with escaped quotes.'\n# Escaped quotes or two strings?\n'is this one string?' 'or two strings?`\n")])])]),e("p",[t._v("The parsing of the second example "),e("em",[t._v("depends")]),t._v(" on that space being between the two string literals.  If we really ignore white-space, then it should parse the same if the space is removed.")]),t._v(" "),e("p",[t._v("Currently, this syntax really means it when it says no white-space, and the second will be a single string.")]),t._v(" "),e("p",[t._v("Another example is identifiers.  In most languages "),e("code",[t._v("foo bar")]),t._v(" is clearly two identifiers while "),e("code",[t._v("foobar")]),t._v(" is one.  In this language, we ignore the space entirely and both look identical to the parser. It needs other clues to know where one identifier/selector starts and where another ends.  This is accomplished by the language having a fixed set of identifiers and careful design to make sure that there is never ambiguity.")]),t._v(" "),e("p",[t._v("For example we should avoid having a "),e("code",[t._v("foo")]),t._v(" selector with short form "),e("code",[t._v("f")]),t._v(" if there is also another selector who's short form is "),e("code",[t._v("o")]),t._v(".  A developer may write "),e("code",[t._v("foo")]),t._v(" and we might interpret it as "),e("code",[t._v("f")]),t._v(", "),e("code",[t._v("o")]),t._v(", "),e("code",[t._v("o")]),t._v(", or they might write "),e("code",[t._v("f o o")]),t._v(" and we interpret it as "),e("code",[t._v("foo")]),t._v(".")]),t._v(" "),e("p",[t._v("This spec does specify which it should be (it would be always "),e("code",[t._v("foo")]),t._v(" because it's longer).  But we also want to avoid these situations whenever possible since humans are not compilers and may have different expectations since most/all existing languages don't really ignore white-space.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t21-43-53z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t21-43-53z-creationix"}},[t._v("#")]),t._v(" (2020-02-10T21:43:53Z) creationix:")]),t._v(" "),e("p",[t._v("@warpfork One solution to the potential confusion with merged identifiers would be to preserve the white-space as tokens and tell the parser about them.  But the problem with this is it would require those spaces to be preserved in compact mode.  This could almost double the length of minimized version and include lots of spaces which can sometimes be problematic in URLs.")]),t._v(" "),e("p",[t._v("I'll consider it further though.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-10t22-10-48z-warpfork"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-10t22-10-48z-warpfork"}},[t._v("#")]),t._v(" (2020-02-10T22:10:48Z) warpfork:")]),t._v(" "),e("p",[t._v('Definitely meant my remark on that as a "2 cents".  You\'ve probably already thought about it much more than I have.')]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-17t22-46-15z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-17t22-46-15z-creationix"}},[t._v("#")]),t._v(" (2020-02-17T22:46:15Z) creationix:")]),t._v(" "),e("p",[t._v("The initial lexer implementation is now done.  The description in the spec for parsing out identifiers seems to be working. https://github.com/creationix/sel-parse-zig/blob/887c3628e11b4ff751e68a9ade4c18a9bf4daf25/src/lexer.zig")]),t._v(" "),e("p",[t._v("In particular, there is a hard coded list of identifiers expected here:")]),t._v(" "),e("div",{staticClass:"language-zig extra-class"},[e("pre",{pre:!0,attrs:{class:"language-zig"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Sorted by longest first, then lexical within same length.")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" identifiers "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"condition"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"recursive"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"recurse"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"fields"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"index"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"match"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"range"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"union"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"all"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"."')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"*"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"~"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"c"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"f"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"i"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"r"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"R"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"u"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),e("p",[t._v("And then parsing those ends up being quite straightforward.")]),t._v(" "),e("div",{staticClass:"language-zig extra-class"},[e("pre",{pre:!0,attrs:{class:"language-zig"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Tokenize Identifiers")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("inline")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("identifiers"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("ident"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" matched "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("true")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[e("span",{pre:!0,attrs:{class:"token builtin-type keyword"}},[t._v("u32")])]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" ident"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("len"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("i "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" input"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("len "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("or")]),t._v(" ident"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("!=")]),t._v(" input"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\nmatched "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("break")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\ni "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("matched"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token class-name"}},[t._v("Token")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("id "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("slice "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" input"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("..")]),t._v("i"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])]),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-02-17t23-08-55z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-02-17t23-08-55z-creationix"}},[t._v("#")]),t._v(" (2020-02-17T23:08:55Z) creationix:")]),t._v(" "),e("p",[t._v("To get an idea of what the lexer tokens look like, this is the output of the following:")]),t._v(" "),e("div",{staticClass:"language-ipldsel extra-class"},[e("pre",{pre:!0,attrs:{class:"language-text"}},[e("code",[t._v("recursive(limit=5\nfields(\n'tree'(\nrecursive(\nall(recurse)\n)\n)\n'parents'(\nall(recurse)\n)\n)\n)\n")])])]),e("div",{staticClass:"language-zig extra-class"},[e("pre",{pre:!0,attrs:{class:"language-zig"}},[e("code",[e("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("       Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   9B      `recursive`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("9")]),t._v("       Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("10")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Unknown      1B      `l`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("11")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   1B      `i`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Unknown      1B      `m`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("13")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   1B      `i`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("14")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Unknown      2B      `t"),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("16")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Decimal      1B      `"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("5")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("17")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   3B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("20")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   6B      `fields`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("26")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("27")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   5B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("32")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String       6B      `'tree'`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("38")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("39")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   7B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("46")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   9B      `recursive`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("55")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("56")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   9B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("65")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   3B      `all`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("68")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("69")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   7B      `recurse`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("76")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("77")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   7B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("84")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("85")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   5B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("90")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("91")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   5B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("96")]),t._v("      Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("String       9B      `'parents'`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("105")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("106")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   7B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("113")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   3B      `all`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("116")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Open 1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("117")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Identifier   7B      `recurse`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("124")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("125")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   5B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("130")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("131")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   3B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("134")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("135")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Whitespace   1B      `\n`\n"),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("136")]),t._v("     Id"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Close        1B      `"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("`\n")])])]),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-02t04-54-53z-rvagg"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-02t04-54-53z-rvagg"}},[t._v("#")]),t._v(" (2020-03-02T04:54:53Z) rvagg:")]),t._v(" "),e("p",[t._v("I really like how the two forms are really just one form, with shortenings (and no comments for URL form), but we're trading that against ease of implementation though, so every environment that needs to be able to use these would need to have a custom parser written. I doesn't "),e("em",[t._v("seem")]),t._v(" difficult, but it's worth noting as we expand our language support (Go, JS and now Rust, but also Filecoin is using selectors and being implemented in C++ and maybe others too and this might be something they want at some point) that we're making that exchange.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-08t03-07-16z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-08t03-07-16z-creationix"}},[t._v("#")]),t._v(" (2020-03-08T03:07:16Z) creationix:")]),t._v(" "),e("p",[t._v("@rvagg FWIW, I'm working on two concurrent implementations.  One in vanilla JS and one using zig-lang which can be compiled to webassembly or a C ABI library for use in virtually any language.  I plan to tweaking this spec with my findings from the two parsers to ensure it's not more difficult than it needs to be.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-14t16-23-40z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-14t16-23-40z-creationix"}},[t._v("#")]),t._v(" (2020-03-14T16:23:40Z) creationix:")]),t._v(" "),e("p",[t._v("So while implementing multiple versions of the parser, I'm getting more and more convinced this spec needs to be more automated and formalized.  I'm now spiking on generating a syntax grammer that can be automatically derived from the selector schema directly.  The design will still be similar to what's proposed here, but it should be more consistent and less hand-crafted to make tooling across the board easier.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-16t14-23-39z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-16t14-23-39z-creationix"}},[t._v("#")]),t._v(" (2020-03-16T14:23:39Z) creationix:")]),t._v(" "),e("p",[t._v('Ok, the JS parser can now correctly compile all the sample selectors in this spec.  The new approach worked well.  Basically, I load the existing IPLD Schema for selectors and generate a parser from that.  In order to match the proposed syntax, the parser generator accepts a list of "aliases" for various types.  For example, this is the line in JS that generates the selector syntax parser:')]),t._v(" "),e("div",{staticClass:"language-js extra-class"},[e("pre",{pre:!0,attrs:{class:"language-js"}},[e("code",[e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" schema "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("schemaParse")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("readFileSync")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token template-string"}},[e("span",{pre:!0,attrs:{class:"token template-punctuation string"}},[t._v("`")]),e("span",{pre:!0,attrs:{class:"token interpolation"}},[e("span",{pre:!0,attrs:{class:"token interpolation-punctuation punctuation"}},[t._v("${")]),t._v("__dirname"),e("span",{pre:!0,attrs:{class:"token interpolation-punctuation punctuation"}},[t._v("}")])]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("/selectors.ipldsch")]),e("span",{pre:!0,attrs:{class:"token template-punctuation string"}},[t._v("`")])]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'utf8'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("const")]),t._v(" parseSelectorEnvelope "),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token function"}},[t._v("makeParser")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("schema"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v('"SelectorEnvelope"')]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("Matcher")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'match'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'.'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreAll")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'all'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'*'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreFields")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'fields'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'f'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreIndex")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'index'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'i'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreRange")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'range'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'r'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreRecursive")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'recursive'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'R'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreUnion")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'union'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'u'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreConditional")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'condition'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'c'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("ExploreRecursiveEdge")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'recurse'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'~'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token literal-property property"}},[t._v("RecursionLimit_None")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'none'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'n'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("The types mentioned are existing in the schema, I'm creating a semi-automated DSL by specifying the entry point and long and short keywords for some types.")]),t._v(" "),e("p",[t._v("Note that this library could be used to create a DSL for "),e("em",[t._v("any")]),t._v(" IPLD data structure that has a schema.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-16t15-10-36z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-16t15-10-36z-creationix"}},[t._v("#")]),t._v(" (2020-03-16T15:10:36Z) creationix:")]),t._v(" "),e("p",[t._v("I found a case where the parentheses are significant and can't always be removed when converting to short form.  Consider the following selector:")]),t._v(" "),e("div",{staticClass:"language-sh extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[t._v("union"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\nunion"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\nmatch\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nmatch\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),e("p",[t._v("Since"),e("code",[t._v("union")]),t._v(" (aka  "),e("code",[t._v("ExploreUnion")]),t._v(") contains a list (aka "),e("code",[t._v("[Selector]")]),t._v("), it consumes an arbitrary number of selectors.  If the parentheses are removed, the two matches will be put under the inner union.  Therefore the short form of this is:")]),t._v(" "),e("div",{staticClass:"language-sh extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Correct short form")]),t._v("\nuu"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("."),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(".\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Wrong short form")]),t._v("\nuu"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("..")]),t._v("\n")])])]),e("p",[t._v("We could keep it simple and say the minimizer always preserves parentheses when encoding a list.  I don't even know of any real world use cases that use "),e("code",[t._v("ExploreUnion")]),t._v(" and would love to see the actual use cases.  A smarter minimizer could do some basic analysis to know when the parentheses are required and only include them then.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-16t15-21-19z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-16t15-21-19z-creationix"}},[t._v("#")]),t._v(" (2020-03-16T15:21:19Z) creationix:")]),t._v(" "),e("p",[t._v("Another case where they are required is labeled matchers inside of fields.")]),t._v(" "),e("div",{staticClass:"language-sh extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Fields with labels")]),t._v("\nfields"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'with-label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\nmatch"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'without-label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\nmatch\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Properly Minimized")]),t._v("\nf"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'with-label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("."),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'without-label'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(".")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Another Properly Minimized")]),t._v("\nf"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'with-label'")]),t._v("."),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),e("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'without-label'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(".")]),t._v("\n\n"),e("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# Broken minimized")]),t._v("\nf"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'with-label'")]),e("span",{pre:!0,attrs:{class:"token builtin class-name"}},[t._v(".")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'label'")]),t._v("'without-label'.\n")])])]),e("p",[t._v("This brings up another question about normalization of minimized form.  Are we OK with there being multiple correct short forms?  Does it matter?")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-16t15-59-25z-creationix"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-16t15-59-25z-creationix"}},[t._v("#")]),t._v(" (2020-03-16T15:59:25Z) creationix:")]),t._v(" "),e("p",[t._v("@rvagg, you were right!")]),t._v(" "),e("p",[t._v("Strings are problematic too.  The less common escaping method used for strings (two single quotes) works, but it also introduces cases where multiple token are merged.  For example, consider the following:")]),t._v(" "),e("div",{staticClass:"language-sh extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[t._v("fields\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'foo'")]),t._v("\nmatch\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("label")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'blue'")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'bar'")]),t._v("\nmatch\n")])])]),e("p",[t._v("This currently breaks because the "),e("code",[t._v("'blue'")]),t._v(" label and the "),e("code",[t._v("'bar'")]),t._v(" field are merged into a single label containing "),e("code",[t._v('"blue\'bar"')]),t._v(".")]),t._v(" "),e("div",{staticClass:"language-sh extra-class"},[e("pre",{pre:!0,attrs:{class:"language-sh"}},[e("code",[t._v("Sample:\n\nfields\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'with-label'")]),t._v("\nmatch\n"),e("span",{pre:!0,attrs:{class:"token assign-left variable"}},[t._v("label")]),e("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'my-label'")]),t._v("\n"),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("'another-field'")]),t._v("\nmatch\n\n\nSyntaxError: "),e("span",{pre:!0,attrs:{class:"token string"}},[t._v("\"fields'foo'matchlabel='blue''bar'match\"")]),t._v("\n^ Unexpected extra syntax at "),e("span",{pre:!0,attrs:{class:"token number"}},[t._v("33")]),t._v("\n")])])]),e("p",[t._v("I propose we switch to a more traditional string syntax with backslash escaping.  It will add some bloat when url encoding, but overall should be an improvement.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-03-17t06-15-41z-rvagg"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-03-17t06-15-41z-rvagg"}},[t._v("#")]),t._v(" (2020-03-17T06:15:41Z) rvagg:")]),t._v(" "),e("blockquote",[e("p",[t._v("This brings up another question about normalization of minimized form. Are we OK with there being multiple correct short forms? Does it matter?")])]),t._v(" "),e("p",[t._v("I don't know specifically for this but we keep on finding cases elsewhere where we don't have one-single-way and this being a problem. I don't know how that would show up here, but maybe if someone chose to encode a selector string rather than a full selector as per schema then having more than one way to say the same thing might be a problem. There's a lot of byte-shavers around that will look at this work and look at the full selector schema and opt for a short string form.")]),t._v(" "),e("p",[t._v("Would it be a big deal to make a "),e("em",[t._v("must be simplest accurate representation")]),t._v(" rule that would give us one-single-way?")]),t._v(" "),e("p",[t._v("Re "),e("code",[t._v("ExploreUnion")]),t._v(", it's a selection strategy that is only useful when you have more than one selector to use isn't it? For practical purposes you should always have >1 item in the list, making parens always necessary and your example overly simple. There are going to be cases of poorly crafted selectors that misuse it, but that shouldn't be the normal case.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-10-08t21-10-58z-mikeal"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-10-08t21-10-58z-mikeal"}},[t._v("#")]),t._v(" (2020-10-08T21:10:58Z) mikeal:")]),t._v(" "),e("p",[t._v("We’ve learned a lot from this but we’re not quite sure how we want to handle simplified string representations for selectors and paths. Closing for now.")]),t._v(" "),e("hr"),t._v(" "),e("h4",{attrs:{id:"_2020-10-08t21-10-58z-closed-by-mikeal"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2020-10-08t21-10-58z-closed-by-mikeal"}},[t._v("#")]),t._v(" (2020-10-08T21:10:58Z) Closed by mikeal")])])}),[],!1,null,null,null);s.default=n.exports}}]);